<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: May 25, 2023 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.042e26407c9e383d96a1f26d6787c686.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=description content="Digested and reproduced from Visualizing A Neural Machine Translation Model by Jay Alammar.
Table of Contents 1. Architecture of Seq2seq 2. Attention Mechanism Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning."><link rel=alternate hreflang=en-us href=https://zhanghanduo.github.io/post/attention/><link rel=canonical href=https://zhanghanduo.github.io/post/attention/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://zhanghanduo.github.io/post/attention/featured.jpg"><meta property="og:site_name" content="Zhang Handuo's Site"><meta property="og:url" content="https://zhanghanduo.github.io/post/attention/"><meta property="og:title" content="Seq2seq Model with Attention | Zhang Handuo's Site"><meta property="og:description" content="Digested and reproduced from Visualizing A Neural Machine Translation Model by Jay Alammar.
Table of Contents 1. Architecture of Seq2seq 2. Attention Mechanism Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning."><meta property="og:image" content="https://zhanghanduo.github.io/post/attention/featured.jpg"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2020-06-01T12:58:12+08:00"><meta property="article:modified_time" content="2020-06-01T12:58:12+08:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://zhanghanduo.github.io/post/attention/"},"headline":"Seq2seq Model with Attention","image":["https://zhanghanduo.github.io/post/attention/featured.jpg"],"datePublished":"2020-06-01T12:58:12+08:00","dateModified":"2020-06-01T12:58:12+08:00","author":{"@type":"Person","name":"Handuo"},"publisher":{"@type":"Organization","name":"Zhang Handuo's Site","logo":{"@type":"ImageObject","url":"https://zhanghanduo.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_3.png"}},"description":"Digested and reproduced from Visualizing A Neural Machine Translation Model by Jay Alammar.\nTable of Contents 1. Architecture of Seq2seq 2. Attention Mechanism Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning."}</script><title>Seq2seq Model with Attention | Zhang Handuo's Site</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=2e16cbd1e0682a2cea47acb2ee507f80><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Zhang Handuo's Site</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Zhang Handuo's Site</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/#featured><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li><li class=nav-item><a class=nav-link href=/uploads/resume.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Seq2seq Model with Attention</h1><div class=article-metadata><div><span>Handuo</span></div><span class=article-date>Jun 1, 2020</span>
<span class=middot-divider></span>
<span class=article-reading-time>7 min read</span>
<span class=middot-divider></span>
<span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/category/deep-learning-basics/>Deep Learning Basics</a></span></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:600px;max-height:114px><div style=position:relative><img src=/post/attention/featured_hucc922ed043f92720762f8fb8b26120c6_15371_720x2500_fit_q75_h2_lanczos.webp width=600 height=114 alt class=featured-image></div></div><div class=article-container><div class=article-style><blockquote><p>Digested and reproduced from <a href=https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/>Visualizing A Neural Machine Translation Model</a> by Jay Alammar.</p></blockquote><details class="toc-inpage d-print-none" open><summary class=font-weight-bold>Table of Contents</summary><nav id=TableOfContents><ul><li><a href=#1-architecture-of-seq2seq>1. Architecture of Seq2seq</a></li><li><a href=#2-attention-mechanism>2. Attention Mechanism</a></li></ul></nav></details><p>Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. <a href=https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/ target=_blank rel=noopener>Google Translate</a> started using such a model in production in late 2016. These models are explained in the two pioneering papers (<a href=https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf target=_blank rel=noopener>Sutskever et al., 2014</a>, <a href=http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf target=_blank rel=noopener>Cho et al., 2014</a>).</p><p>A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images&mldr;etc) and outputs another sequence of items.</p><p>In neural machine translation, a sequence is a series of words, processed one after another. The output is, likewise, a series of words:</p><video controls>
<source src=/post/attention/seq2seq_2.mp4 type=video/mp4></video><h2 id=1-architecture-of-seq2seq>1. Architecture of Seq2seq</h2><p>The model is composed of an
<mark>encoder</mark> and a
<mark>decoder</mark>. The
<mark>encoder</mark> processes each item in the input sequence and compiles the information into a vector (called the
<mark>context</mark> ). After processing the input sequence, the
<mark>encoder</mark> sends the
<mark>context</mark> over to the
<mark>decoder</mark> , which produces the output sequence item by item.</p><video controls>
<source src=/post/attention/seq2seq_3.mp4 type=video/mp4></video><p>The same applies in the case of machine translation.</p><video controls>
<source src=/post/attention/seq2seq_4.mp4 type=video/mp4></video><p>RNN (recurrent neural network) is shown as an illustration here to be the model of both
<mark>encoder</mark> and
<mark>decoder</mark> (Be sure to check out Luis Serrano&rsquo;s <a href="https://www.youtube.com/watch?v=UNmqTiOnRfg" target=_blank rel=noopener>A friendly introduction to Recurrent Neural Networks</a> for an intro to RNNs).</p><figure id=figure-the-context-is-a-vector-of-floats-later-in-this-post-we-will-visualize-vectors-in-color-by-assigning-brighter-colors-to-the-cells-with-higher-values><div class="d-flex justify-content-center"><div class=w-100><img alt="The **context** is a vector of floats. Later in this post we will visualize vectors in color by assigning brighter colors to the cells with higher values." srcset="/post/attention/context_hu7a6233c42fac561492b0af65e3f78395_20402_85ccdd37ca6d815d76d50e0b8096cd4f.webp 400w,
/post/attention/context_hu7a6233c42fac561492b0af65e3f78395_20402_7f20bef431fa1ca8c6bf6e5882d78875.webp 760w,
/post/attention/context_hu7a6233c42fac561492b0af65e3f78395_20402_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/attention/context_hu7a6233c42fac561492b0af65e3f78395_20402_85ccdd37ca6d815d76d50e0b8096cd4f.webp width=716 height=252 loading=lazy data-zoomable></div></div><figcaption>The <strong>context</strong> is a vector of floats. Later in this post we will visualize vectors in color by assigning brighter colors to the cells with higher values.</figcaption></figure><p>You can set the size of the
<mark>context</mark> vector when you set up your model. It is basically the number of hidden units in the
<mark>encoder</mark> RNN. These visualizations show a vector of size 4, but in real world applications the
<mark>context</mark> vector would be of a size like 256, 512, or 1024.</p><p>By design, a RNN takes two inputs at each time step: an input (in the case of the encoder, one word from the input sentence), and a hidden state. The word, however, needs to be represented by a vector. To transform a word into a vector, we turn to the class of methods called &ldquo;<a href=https://machinelearningmastery.com/what-are-word-embeddings/ target=_blank rel=noopener>word embedding</a>&rdquo; algorithms. These turn words into vector spaces that capture a lot of the meaning/semantic information of the words (e.g. <a href=http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html target=_blank rel=noopener>king - man + woman = queen</a>).</p><figure id=figure-we-need-to-turn-the-input-words-into-vectors-before-processing-them-that-transformation-is-done-using-a-word-embedding-algorithm-we-can-use-pre-trained-embeddingshttpahogrammercom20170120the-list-of-pretrained-word-embeddings-or-train-our-own-embedding-on-our-dataset-embedding-vectors-of-size-200-or-300-are-typical-were-showing-a-vector-of-size-four-for-simplicity><div class="d-flex justify-content-center"><div class=w-100><img alt="We need to turn the input words into vectors before processing them. That transformation is done using a **word embedding** algorithm. We can use [pre-trained embeddings](http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/) or train our own embedding on our dataset. Embedding vectors of size 200 or 300 are typical, we're showing a vector of size four for simplicity." srcset="/post/attention/embedding_hue52ea6887e4ffd78961218df5eff4111_41024_4caff9934379c54892ef9e2a484b83fd.webp 400w,
/post/attention/embedding_hue52ea6887e4ffd78961218df5eff4111_41024_80b70f6d1ed50e6368f83f94c6f9100d.webp 760w,
/post/attention/embedding_hue52ea6887e4ffd78961218df5eff4111_41024_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/attention/embedding_hue52ea6887e4ffd78961218df5eff4111_41024_4caff9934379c54892ef9e2a484b83fd.webp width=760 height=248 loading=lazy data-zoomable></div></div><figcaption>We need to turn the input words into vectors before processing them. That transformation is done using a <strong>word embedding</strong> algorithm. We can use <a href=http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/ target=_blank rel=noopener>pre-trained embeddings</a> or train our own embedding on our dataset. Embedding vectors of size 200 or 300 are typical, we&rsquo;re showing a vector of size four for simplicity.</figcaption></figure><p>Now that we&rsquo;ve introduced our main vectors/tensors, let&rsquo;s recap the mechanics of an RNN and establish a visual language to describe these models:</p><video controls>
<source src=/post/attention/RNN_1.mp4 type=video/mp4></video><p>The next RNN step takes the second input vector and hidden state #1 to create the output of that time step. Later in the post, we&rsquo;ll use an animation like this to describe the vectors inside a neural machine translation model.</p><p>In the following visualization, each pulse for the
<mark>encoder</mark> or
<mark>decoder</mark> is that RNN processing its inputs and generating an output for that time step. Since the
<mark>encoder</mark> and
<mark>decoder</mark> are both RNNs, each time step one of the RNNs does some processing, it updates its
<mark>hidden state</mark> based on its inputs and previous inputs it has seen.</p><p>Let&rsquo;s look at the
<mark>hidden states</mark> for the
<mark>encoder</mark>. Notice how the last
<mark>hidden state</mark> is actually the
<mark>context</mark> we pass along to the
<mark>decoder</mark>.</p><video controls>
<source src=/post/attention/seq2seq_5.mp4 type=video/mp4></video><p>The
<mark>decoder</mark> also maintains a
<mark>hidden states</mark> that it passes from one time step to the next. We just didn&rsquo;t visualize it in this graphic because we&rsquo;re concerned with the major parts of the model for now.</p><p>Let&rsquo;s now look at another way to visualize a sequence-to-sequence model. This animation will make it easier to understand the static graphics that describe these models. This is called an &ldquo;unrolled&rdquo; view where instead of showing the one
<mark>decoder</mark>, we show a copy of it for each time step. This way we can look at the inputs and outputs of each time step.</p><video controls>
<source src=/post/attention/seq2seq_6.mp4 type=video/mp4></video><h2 id=2-attention-mechanism>2. Attention Mechanism</h2><p>Attention is a generalized pooling method with. The core component in the attention mechanism is the attention layer, or called attention for simplicity. An input of the attention layer is called a query. For a query, attention returns an o bias alignment over inputsutput based on the memory — a set of key-value pairs encoded in the attention layer. To be more specific, assume that the memory contains $n$ key-value pairs, $(k_1, v_1), \cdots, (k_n, v_n)$ with $\mathbf{k}\in{ \mathbb{R}^{d_k}}, \mathbf{v}\in{ \mathbb{R}^{d_v}}$. Given a query $\mathbf{q}\in \mathbb{R}^{d_q}$, the attention layer returns an output $\mathbf{o}\in{ \mathbb{R}^{d_v}} $ with the same shape as the value.</p><p>Pros of using attention:</p><ol><li>With attention, Seq2seq does not forget the source input.</li><li>With attention, the decoder knows where to focus.</li></ol><p>The
<mark>context</mark> vector turned out to be a bottleneck for these types of models. It made it challenging for the models to deal with long sentences. A solution was proposed in <a href=https://arxiv.org/abs/1409.0473 target=_blank rel=noopener>Bahdanau et al., 2014</a> and <a href=https://arxiv.org/abs/1508.04025 target=_blank rel=noopener>Luong et al., 2015</a>. These papers introduced and refined a technique called &ldquo;Attention&rdquo;, which highly improved the quality of machine translation systems. Attention allows the model to focus on the relevant parts of the input sequence as needed.</p><figure id=figure-at-time-step-7-the-attention-mechanism-enables-the-decoder-to-focus-on-the-word-étudiant-student-in-french-before-it-generates-the-english-translation-this-ability-to-amplify-the-signal-from-the-relevant-part-of-the-input-sequence-makes-attention-models-produce-better-results-than-models-without-attention><div class="d-flex justify-content-center"><div class=w-100><img alt="At time step 7, the attention mechanism enables the **decoder** to focus on the word **étudiant** (*student* in french) before it generates the English translation. This ability to amplify the signal from the relevant part of the input sequence makes attention models produce better results than models without attention." srcset="/post/attention/attention_hu3a304cc6d98049bbd17d98bab91c60b0_73300_e78158aec6a1e57573114c9c6faf85a1.webp 400w,
/post/attention/attention_hu3a304cc6d98049bbd17d98bab91c60b0_73300_3aad1825d2b3ab1c14408a73d81a281c.webp 760w,
/post/attention/attention_hu3a304cc6d98049bbd17d98bab91c60b0_73300_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/attention/attention_hu3a304cc6d98049bbd17d98bab91c60b0_73300_e78158aec6a1e57573114c9c6faf85a1.webp width=760 height=209 loading=lazy data-zoomable></div></div><figcaption>At time step 7, the attention mechanism enables the <strong>decoder</strong> to focus on the word <strong>étudiant</strong> (<em>student</em> in french) before it generates the English translation. This ability to amplify the signal from the relevant part of the input sequence makes attention models produce better results than models without attention.</figcaption></figure><p>Let&rsquo;s continue looking at attention models at this high level of abstraction. An attention model differs from a classic sequence-to-sequence model in two main ways:</p><p>First, the encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes <em>all</em> the hidden states to the decoder:</p><video controls>
<source src=/post/attention/seq2seq_7.mp4 type=video/mp4></video><p>Second, an attention decoder does an extra step before producing its output. In order to focus on the parts of the input that are relevant to this decoding time step, the decoder does the following:</p><ol><li>Look at the set of encoder
<mark>hidden states</mark> it received &ndash; each encoder hidden states is most associated with a certain word in the input sentence</li><li>Give each
<mark>hidden states</mark> a score (let&rsquo;s ignore how the scoring is done for now)</li><li>Multiply each
<mark>hidden states</mark> by its softmaxed score, thus amplifying
<mark>hidden states</mark> with high scores, and drowning out
<mark>hidden states</mark> with low scores</li></ol><video controls>
<source src=/post/attention/attention_process.mp4 type=video/mp4></video><p>This scoring exercise is done at each time step on the
<mark>decoder</mark> side.</p><p>Let us now bring the whole thing together in the following visualization and look at how the attention process works:</p><ol><li>The attention decoder RNN takes in the embedding of the &lt;END> token, and an initial decoder hidden state.</li><li>The RNN processes its inputs, producing an output and a new hidden state vector (h4). The output is discarded.</li><li>Attention Step: We use the encoder hidden states and the h4 vector to calculate a context vector (C4) for this time step.</li><li>We concatenate h4 and C4 into one vector.</li><li>We pass this vector through a
<mark>feedforward neural network</mark> (one trained jointly with the model).</li><li>The output of the feedforward neural networks indicates the output word of this time step.</li><li>Repeat for the next time steps</li></ol><video controls>
<source src=/post/attention/attention_tensor_dance.mp4 type=video/mp4></video><p>This is another way to look at which part of the input sentence we&rsquo;re paying attention to at each decoding step:</p><video controls>
<source src=/post/attention/seq2seq_9.mp4 type=video/mp4></video><p>Note that the model isn&rsquo;t just mindless aligning the first word at the output with the first word from the input. It actually learned from the training phase how to align words in that language pair (French and English in our example). An example for how precise this mechanism can be comes from the attention papers listed above:</p><figure id=figure-you-can-see-how-the-model-paid-attention-correctly-when-outputing-european-economic-area-in-french-the-order-of-these-words-is-reversed-européenne-économique-zone-as-compared-to-english-every-other-word-in-the-sentence-is-in-similar-order><div class="d-flex justify-content-center"><div class=w-100><img alt="You can see how the model paid attention correctly when outputing **European Economic Area**. In French, the order of these words is reversed (*européenne économique zone*) as compared to English. Every other word in the sentence is in similar order." srcset="/post/attention/attention_sentence_hu9a38ed3c48ed5954b46a7f19ef9a39e3_54758_10bfeee107fcc0b7e4143cc0ba0d88a2.webp 400w,
/post/attention/attention_sentence_hu9a38ed3c48ed5954b46a7f19ef9a39e3_54758_5650b663e87811862988e2ac79414855.webp 760w,
/post/attention/attention_sentence_hu9a38ed3c48ed5954b46a7f19ef9a39e3_54758_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/attention/attention_sentence_hu9a38ed3c48ed5954b46a7f19ef9a39e3_54758_10bfeee107fcc0b7e4143cc0ba0d88a2.webp width=634 height=647 loading=lazy data-zoomable></div></div><figcaption>You can see how the model paid attention correctly when outputing <strong>European Economic Area</strong>. In French, the order of these words is reversed (<em>européenne économique zone</em>) as compared to English. Every other word in the sentence is in similar order.</figcaption></figure><p>You can check TensorFlow&rsquo;s <a href=https://github.com/tensorflow/nmt target=_blank rel=noopener>Neural Machine Translation (seq2seq) Tutorial</a>.</p></div><div class=article-tags><a class="badge badge-light" href=/tag/ml/>ML</a>
<a class="badge badge-light" href=/tag/notes/>Notes</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fzhanghanduo.github.io%2Fpost%2Fattention%2F&amp;text=Seq2seq+Model+with+Attention" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fzhanghanduo.github.io%2Fpost%2Fattention%2F&amp;t=Seq2seq+Model+with+Attention" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Seq2seq%20Model%20with%20Attention&amp;body=https%3A%2F%2Fzhanghanduo.github.io%2Fpost%2Fattention%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fzhanghanduo.github.io%2Fpost%2Fattention%2F&amp;title=Seq2seq+Model+with+Attention" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Seq2seq+Model+with+Attention%20https%3A%2F%2Fzhanghanduo.github.io%2Fpost%2Fattention%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fzhanghanduo.github.io%2Fpost%2Fattention%2F&amp;title=Seq2seq+Model+with+Attention" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2023 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.1d4346c6f7d46c340dc0a9058dd85c13.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/en/js/wowchemy.min.c84202fca2a6efbbecbaf0e8358c1d51.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>