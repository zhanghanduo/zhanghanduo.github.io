[{"authors":["admin"],"categories":null,"content":" Dr. Zhang Handuo is currently the AI Scientist at Mind Pointeye leading a group of 7 engineers. My research interests include Robot Perception and computer Vision. Currently I am working on human-level spatial awareness and reasoning with the aid of AI.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Dr. Zhang Handuo is currently the AI Scientist at Mind Pointeye leading a group of 7 engineers. My research interests include Robot Perception and computer Vision. Currently I am working on human-level spatial awareness and reasoning with the aid of AI.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemyâ€™s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://zhanghanduo.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Handuo"],"categories":["Deep Learning Basics"],"content":" Digested and reproduced from Visualizing A Neural Machine Translation Model by Jay Alammar.\nTable of Contents 1. Architecture of Seq2seq 2. Attention Mechanism Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started using such a model in production in late 2016. These models are explained in the two pioneering papers (Sutskever et al., 2014, Cho et al., 2014).\nA sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an imagesâ€¦etc) and outputs another sequence of items.\nIn neural machine translation, a sequence is a series of words, processed one after another. The output is, likewise, a series of words:\n1. Architecture of Seq2seq The model is composed of an encoder and a decoder. The encoder processes each item in the input sequence and compiles the information into a vector (called the context ). After processing the input sequence, the encoder sends the context over to the decoder , which produces the output sequence item by item.\nThe same applies in the case of machine translation.\nRNN (recurrent neural network) is shown as an illustration here to be the model of both encoder and decoder (Be sure to check out Luis Serranoâ€™s A friendly introduction to Recurrent Neural Networks for an intro to RNNs).\nThe context is a vector of floats. Later in this post we will visualize vectors in color by assigning brighter colors to the cells with higher values. You can set the size of the context vector when you set up your model. It is basically the number of hidden units in the encoder RNN. These visualizations show a vector of size 4, but in real world applications the context vector would be of a size like 256, 512, or 1024.\nBy design, a RNN takes two inputs at each time step: an input (in the case of the encoder, one word from the input sentence), and a hidden state. The word, however, needs to be represented by a vector. To transform a word into a vector, we turn to the class of methods called â€œword embeddingâ€ algorithms. These turn words into vector spaces that capture a lot of the meaning/semantic information of the words (e.g. king - man + woman = queen).\nWe need to turn the input words into vectors before processing them. That transformation is done using a word embedding algorithm. We can use pre-trained embeddings or train our own embedding on our dataset. Embedding vectors of size 200 or 300 are typical, weâ€™re showing a vector of size four for simplicity. Now that weâ€™ve introduced our main vectors/tensors, letâ€™s recap the mechanics of an RNN and establish a visual language to describe these models:\nThe next RNN step takes the second input vector and hidden state #1 to create the output of that time step. Later in the post, weâ€™ll use an animation like this to describe the vectors inside a neural machine translation model.\nIn the following visualization, each pulse for the encoder or decoder is that RNN processing its inputs and generating an output for that time step. Since the encoder and decoder are both RNNs, each time step one of the RNNs does some processing, it updates its hidden state based on its inputs and previous inputs it has seen.\nLetâ€™s look at the hidden states for the encoder. Notice how the last hidden state is actually the context we pass along to the decoder.\nThe decoder also maintains a hidden states that it passes from one time step to the next. We just didnâ€™t visualize it in this graphic because weâ€™re concerned with the major parts of the model for now.\nLetâ€™s now look at another way to visualize a sequence-to-sequence model. This animation will make it easier to understand the static graphics that describe these models. This is called an â€œunrolledâ€ view where instead of showing the one decoder, we show a copy of it for each time step. This way we can look at the inputs and outputs of each time step.\n2. Attention Mechanism Attention is a generalized pooling method with. The core component in the attention mechanism is the attention layer, or called attention for simplicity. An input of the attention layer is called a query. For a query, attention returns an o bias alignment over inputsutput based on the memory â€” a set of key-value pairs encoded in the attention layer. To be more specific, assume that the memory contains $n$ key-value pairs, $(k_1, v_1), \\cdots, (k_n, v_n)$ with $\\mathbf{k}\\in{ \\mathbb{R}^{d_k}}, \\mathbf{v}\\in{ \\mathbb{R}^{d_v}}$. Given a query $\\mathbf{q}\\in \\mathbb{R}^{d_q}$, the attention layer returns an output $\\mathbf{o}\\in{ \\mathbb{R}^{d_v}} $ with the same shape as the value.\nPros of using attention:\nWith attention, Seq2seq does not forget the source input. With attention, the decoder knows where to focus. The context vector turned out to be a bottleneck for these types of models. It made it challenging for the models to deal with long sentences. A solution was proposed in Bahdanau et â€¦","date":1590987492,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590987492,"objectID":"2e16cbd1e0682a2cea47acb2ee507f80","permalink":"https://zhanghanduo.github.io/post/attention/","publishdate":"2020-06-01T12:58:12+08:00","relpermalink":"/post/attention/","section":"post","summary":"Digested and reproduced from Visualizing A Neural Machine Translation Model by Jay Alammar.\nTable of Contents 1. Architecture of Seq2seq 2. Attention Mechanism Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning.","tags":["ML","Notes"],"title":"Seq2seq Model with Attention","type":"post"},{"authors":null,"categories":null,"content":"Stereo Vision System on High Speed Unmanned Ground Vehicle Purpose of this project to develop a stereo vision system to explore surroundings for obstacle detection, tracking and mapping; object classification; visual SLAM, road feature detection (like slope detection, curb detection and lane detection). Please visit our project page for more details.\n","date":1566259200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566259200,"objectID":"de0ac8cce1f695e96f01337d58e08e52","permalink":"https://zhanghanduo.github.io/project/ugv/","publishdate":"2019-08-20T00:00:00Z","relpermalink":"/project/ugv/","section":"project","summary":"Stereo Vision System on High Speed Unmanned Ground Vehicle.","tags":["Deep Learning"],"title":"Stereo Vision System on UGV","type":"project"},{"authors":["Karunasekera Hasith","Han Wang","Handuo Zhang"],"categories":null,"content":"","date":1565169150,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565169150,"objectID":"ee6272850888001bf124f8ab45d8501d","permalink":"https://zhanghanduo.github.io/publication/multiple_tracking/","publishdate":"2019-08-07T17:12:30+08:00","relpermalink":"/publication/multiple_tracking/","section":"publication","summary":"Objective of multiple object tracking (MOT) is to assign a unique track identity for all the objects of interest in a video, across the whole sequence. Tracking-by-detection is the most common approach used in addressing MOT problem. In this work, we propose a method to address MOT by defining a dissimilarity measure based on object motion, appearance, structure, and size. We calculate the appearance and structure-based dissimilarity measure by matching histograms following a grid architecture. Motion and size for each track are predicted using the information from trackâ€™s history. These dissimilarity values are then used in the Hungarian algorithm, in the data association step for track identity assignment. In addition, we introduce a method to address any false detection  stable tracks. The proposed method runs in real time following an online approach. We evaluate our method in both MOT17 benchmark data-set for pedestrian tracking and KITTI benchmark data-set for vehicle tracking using the same system parameters to verify the robustness of the proposed method. The method can achieve state-of-the-art results in both benchmarks.","tags":["Tracking"],"title":"Multiple Object Tracking With Attention to Appearance, Structure, Motion and Size","type":"publication"},{"authors":["Handuo"],"categories":[],"content":"Remotely editing your work when your server does not have public IP address and you donâ€™t want to spend any money is not so easy. Maybe you can use Team viewer or Anydesk or even chrome remote desktop, but there are high latencies. Maybe you can use ngrok to remotely ssh to your server, you have to use vim and you are not familiar with it at all ğŸ˜§. I tried to use rmate but it is not convinient to edit across different files in a folder.\nI recently found an hot github repository called code-server which is able to run VS Code on a remote server, accessible through the browser. So it suddenly came to my mind that I can remotely edit any code for free as long as I have a linux/macOS environment.\nLetâ€™s consider you understand the basic knowledge of SSH key as you are going to use it. For tutorial about how to generate SSH keys, please refer to How to set up SSH keys and connecting to GitHub with SSH.\nFirstly you need to see whether your server has a public IP address. If yes (I know this is not common), then things are really easy and you can just follow step 1, 2 and 3; otherwise, directly go to step 0, then step 2 and 3.\nStep 1. SSH connect ssh server_username@IP_address -L 8843:localhost:8843 The -L 8843:localhost:8843 here is local port forwarding which allows you to access local network resources that arenâ€™t exposed to the Internet. The first 8843 is local port, localhost:8843 is the remote code-server default port.\nTo see whether you can successfully links to the server. The prerequisites are 1) you installed openssh-client 2) you have generated SSH key. If not successfully, maybe you donâ€™t have a public IP address. Then go to step 0.\nStep 2. Download code-server Open this page on your client browser, find the latest release of code-server. Find the binary file for linux and get the downloading address. Then in the terminal window ssh connected to the remote server, type:\nwget code-server_downloading_address # Example: wget https://github.com/codercom/code-server/releases/download/1.696-vsc1.33.0/code-server1.696-vsc1.33.0-linux-x64.tar.gz tar -xvzf code-server1.696-vsc1.33.0-linux-x64.tar.gz cd code-server1.696-vsc1.33.0-linux-x64/ sudo mv code-server /usr/local/bin/ Then your code-server will be installed!\nStep 3. Running code-server Go to the folder of your code waiting to be edited and type code-server in the terminal window ssh connected to remote server. Then open your browser and type localhost:8843, your workspace of VSCode will be revealed to you! The speed is satisfactory to me.\nStep 0. Ngrok Some people will use VPS servers or cloud hosting providers like Vultr, AWS and so on to pay for a public IP address. But here we just need Ngrok, a great tool that can create a tunnel from the public Internet to a port on your local machine. You can give this URL to anyone and any place without the need to pay any money!\nDownload ngrok onto your remote server and throw the binary file into /usr/local/bin/. Maybe need to sudo chmod a+x ngrok. Then type:\nngrok tcp 22 --region ap where â€“region refers to your region. There are four region options: us(Ohio), eu(Frankfurt), ap(Singapore), au(Sydney). If you donâ€™t select a region, the default one is us, which might be slow if you are in Asia.\nThen your screen will show something like this: There is a number after 0.tcp.ngrok.io: 15707. You need to remember this port number. Please keep this window on if you want to keep this tunnel open.\nThen you can ssh to your remote server by copying the command:\nssh hd@0.tcp.ngrok.io -p15707 -L 8443:localhost:8443 #Or your region is ap ssh hd@0.tcp.ap.ngrok.io -p15707 -L 8443:localhost:8443 ","date":1554794437,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554794437,"objectID":"e44aa243c1a13e1a384b7e00b0e807be","permalink":"https://zhanghanduo.github.io/post/remote_edit/","publishdate":"2019-04-09T15:20:37+08:00","relpermalink":"/post/remote_edit/","section":"post","summary":"Remotely editing your work when your server does not have public IP address and you donâ€™t want to spend any money is not so easy. Maybe you can use Team viewer or Anydesk or even chrome remote desktop, but there are high latencies.","tags":["Network","Tools"],"title":"How to remotely edit your project without having to use VIM","type":"post"},{"authors":["Handuo Zhang","Karunasekera Hasith","Han Wang"],"categories":null,"content":"","date":1551949950,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551949950,"objectID":"ef1a4715871aa05f4ffeed94f55b2f95","permalink":"https://zhanghanduo.github.io/publication/gmc/","publishdate":"2019-03-07T17:12:30+08:00","relpermalink":"/publication/gmc/","section":"publication","summary":"Conventional SLAM algorithms takes a strong assumption of scene motionlessness, which limits the application in real environments. This paper tries to tackle the challenging visual SLAM issue of moving objects in dynamic environments. We present GMC, grid-based motion clustering approach, a lightweight dynamic object filtering method that is free from high-power and expensive processors. GMC encapsulates motion consistency as the statistical likelihood of detected key points within a certain region. Using this method can we provide real-time and robust correspondence algorithm that can differentiate dynamic objects with static backgrounds. We evaluate our system in public TUM dataset. To compare with the state-of-the-art methods, our system can provide more accurate results by detecting dynamic objects.","tags":["SLAM"],"title":"GMC: Grid Based Motion Clustering in Dynamic Environment","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Letâ€™s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://zhanghanduo.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Handuo"],"categories":[],"content":"When you want to install a brand new Ubuntu 16.04 system. You could try to follow this guidance.\nOpen Software \u0026amp; Updates and choose the fastest source.\nUpdate the system:\nsudo sh -c \u0026#39;echo \u0026#34;deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\u0026#34; \u0026gt; /etc/apt/sources.list.d/ros-latest.list\u0026#39; sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 421C365BD9FF1F717815A3895523BAEEB01FA116 sudo add-apt-repository ppa:graphics-drivers/ppa sudo apt-get update sudo apt-get dist-upgrade sudo apt-get install build-essential git Install Nvidia driver sudo apt-get install nvidia-396 nvidia-settings Under cases you have Intel GPU also, please type:\nsudo prime-select nvidia Clone the dotfile repository: mkdir softwares \u0026amp;\u0026amp; cd softwares https://github.com/zhanghanduo/dotfiles.git cd dotfiles Then follow the readme.md to install necessary packages and change some settings. Then reboot to make some settings valid.\nInstall CUDA: Type:\nsudo chmod a+x cuda_xxx_linux_64.run sudo ./cuda_xxx_linux_64.run add the following settings to your .zshrc or .bashrc file and source it:\nexport PATH=$PATH:/usr/local/cuda/bin export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH export CUDA_HOME=/usr/local/cuda Install CUDNN unzip the CUDNN file:\ntar -xzvf cudnn-9.0-linux-x64-v7.tgz Then copy and change permissions:\nsudo cp cuda/include/cudnn.h /usr/local/cuda/include sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64 sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* Install ROS Kinetic sudo apt-get install ros-kinetic-desktop-full sudo rosdep init rosdep update sudo apt-get install python-rosinstall python-rosinstall-generator python-wstool add the following settings to your .zshrc file and ** source ** it:\nsource /opt/ros/kinetic/setup.zsh source ~/.zshrc If you use bash instead of zsh you need to run the following commands to set up your shell:\nsource /opt/ros/kinetic/setup.bash source ~/.bashrc Instal OpenCV 3.x 8.1 Download OpenCV source codes can be downloaded in opencv github. OpenCV corresponding contrib can be downloaded in opencv_contrib github.\n8.2 Prerequisites\nBefore you install OpenCV\nmkdir release \u0026amp;\u0026amp; cd release sudo apt-get build-dep -y opencv (Optional) sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev 8.3 Cmake\nFor normal configure, just type:\ncmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D WITH_TBB=ON -D WITH_V4L=ON -D ENABLE_CXX11=ON -D ENABLE_PRECOMPILED_HEADERS=OFF -D CMAKE_CXX_FLAGS=\u0026#39;-std=c++11\u0026#39; .. With contrib, just add: -D OPENCV_EXTRA_MODULES_PATH=/home/hd/softwares/opencv_contrib-3.4.1/modules.\nWith CUDA, just add: -D WITH_CUDA=ON -D ENABLE_FAST_MATH=1 -D CUDA_FAST_MATH=1 -D WITH_CUBLAS=1 -D CUDA_GENERATION=Auto.\nIf you encounter some C++ 11 errors during compiling when you install 3.4 or later, just add: -D CMAKE_CXX_FLAGS=\u0026#39;-std=c++11\u0026#39; -D CUDA_NVCC_FLAGS=\u0026#39;-std=c++11 --expt-relaxed-constexpr\u0026#39;\n8.4 Compiling and installing\nmake -j8 sudo make install 8.5 Environment\necho \u0026#39;/usr/local/lib\u0026#39; | sudo tee -a /etc/ld.so.conf.d/opencv.conf sudo ldconfig printf \u0026#39;# OpenCV\\nPKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig\\nexport PKG_CONFIG_PATH\\n\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.zshrc pkg-config --modversion opencv\t//check if opencv is installed and its version ","date":1540795574,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540795574,"objectID":"6956809c88f93ba430435f11a7a084d4","permalink":"https://zhanghanduo.github.io/post/new_system/","publishdate":"2018-10-29T14:46:14+08:00","relpermalink":"/post/new_system/","section":"post","summary":"When you want to install a brand new Ubuntu 16.04 system. You could try to follow this guidance.\nOpen Software \u0026 Updates and choose the fastest source.\nUpdate the system:","tags":["Info"],"title":"Install new linux environment","type":"post"},{"authors":["Handuo"],"categories":[],"content":"I summarized several cloud based GPU services:\nName of services Specification Price (US$) AWS P2 instance p2.xLarge 0.9 / hour Azure NC6 1xK80 0.9 / hour Lambda GPU cloud 8x AWS P2 instances 0.90 / GPU/ hour NTU HPCC 2 units of 1-P100 is scheduled to be ready by End of October 0.78 / core/ hour ","date":1540793863,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540793863,"objectID":"8548358de7da538a174c8a2dd87ee82a","permalink":"https://zhanghanduo.github.io/post/gpu_cloud/","publishdate":"2018-10-29T14:17:43+08:00","relpermalink":"/post/gpu_cloud/","section":"post","summary":"I summarized several cloud based GPU services:\nName of services Specification Price (US$) AWS P2 instance p2.xLarge 0.9 / hour Azure NC6 1xK80 0.9 / hour Lambda GPU cloud 8x AWS P2 instances 0.","tags":["Info"],"title":"Price and spec of cloud based GPU","type":"post"},{"authors":["Handuo"],"categories":["Object Detection"],"content":"YOLO has higher localization errors and the recall (measure how good to locate all objects) is lower, compared to SSD. YOLOv2 is the second version of the YOLO with the objective of improving the accuracy significantly while making it faster.\nThe backbone network architecture of YOLO v2 is as follows: 1. Accuracy Improvements Batch Normalization Also removes the need of dropouts. mAP increases by 2%.\nHigh-resolution Classifier To generate predictions with shape of $7\\times 7 \\times 125$, we replace the final fully connected layers with a $3\\times 3$ convolution layer each outputting 1024 output channels. Then we apply a final $1\\times 1$ convolutional layer to convert the $7\\times 7 \\times 1024$ output into $7\\times 7 \\times 125$ and retrain it end-to-end. This makes training easier and moves mAP up by 4%.\nConvolution with Anchor Boxes Early training is susceptible to unstable gradients. Arbitrary guesses on the boundary boxes may result in steep gradient changes.\nIn real life, boudnary boxes are not arbitrary. So the author create 5 anchor boxes with the following shapes.\nInstead of directly predicting 5 arbitrary boundary boxes, we predict offsdets to each of the anchor boxes. If we constrain the offset values, we can maintain the diversity of the predictions and have each prediction focusing on specific shape. So the initial training will be more stable.\nDimension Clusters In many problem domains, the boundary boxes have strong patterns. For example, in the autonomous driving, the 2 most common boundary boxes will be cars and pedestrians at different distances. To identify the top-K boundary boxes that have the best coverage for the training data, we run K-means clustering on the training data to locate the centroids of the top-K clusters.\nSince we are dealing with boundary boxes rather than points, we cannot use the regular spatial distance to measure datapoint distances. No surprise, we use IoU.\nDirect location prediction We make predictions on the offsets to the anchors. Nevertheless, if it is unconstrained, our guesses will be randomized again. YOLO predicts 5 parameters (tx, ty, tw, th, and to) and applies the sigma function to constraint its possible offset range.\nWith the use of k-means clustering (dimension clusters) and the improvement mentioned in this section, mAP increases 5%.\n","date":1534741092,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534741092,"objectID":"c629cddfc26d613b00ba7d6e7a7e73af","permalink":"https://zhanghanduo.github.io/post/yolov2/","publishdate":"2018-08-20T12:58:12+08:00","relpermalink":"/post/yolov2/","section":"post","summary":"YOLO has higher localization errors and the recall (measure how good to locate all objects) is lower, compared to SSD. YOLOv2 is the second version of the YOLO with the objective of improving the accuracy significantly while making it faster.","tags":["ML","Notes"],"title":"You only look once (YOLO) -- (2)","type":"post"},{"authors":["Handuo"],"categories":["Object Detection"],"content":"You Only Look Once (YOLO) is an object detection system targeted for real-time processing. There are three versions of YOLO: YOLO, YOLOv2 (and YOLO9000) and YOLOv3. For this article, we mainly focus on YOLO first stage.\n1. Introduction The target is to find out the bounding box (rectangular boundary frame) of all the objects in the picture and meanwhile judge the categories of them, where left top coordinate denoted by $(x,y)$, as well as the width and height of the rectangle bounding box by $(w,h)$. The challenge here is that we have unknown number of objects, so the output dimension is not fixed.\n2. Grid Cell The idea of YOLO is to output a fixed number of dimension which is big enough to contain all the objects. We crop the original picture and divide it into an $S\\times S$ grid. Each grid cell predicts only one object. For example, the red grid cell tries to predict the â€œdogâ€ object whose center falls inside that grid cell. Each grid cell predicts a fixed number of boundary boxes. In the next example, the yellow grid cell makes two boundary box predictions (blue boxes) to locate where the person is. For each grid cell,\nit predicts B boundary boxes and each box has a box confidence score, it detects one object only regardless of the number of boxes B, it predicts C conditional class probabilities. For example we can use $7\\times 7$ grids ($S\\times S$), 2 boundary boxes (B) with 1 corresponding confidence score and 4 coordinates ($w,h,x,y$), as well as 4 classes (C), which makes up for $1\\times 14$ tensor.\n3. Network Design YOLO has 24 convolutional layers followed by 2 fully connected layers (FC). Some convolution layers use $1\\times 1$ reduction layers to reduce the depth of feature maps. For the last convolution layer, the output is a tensor with shape $(7,7,1024). Then tensor is flattened, and finally output $7\\times 7 \\times 30$ parameters (if 20 classes and 2 bboxes predictions per grid cell) through linear regression. 4. Loss Fuction YOLO uses sum-squared error between predictions (the one with highest IoU) and ground truth to calculate loss. The loss function composes of:\nthe classification loss. the localization loss. the confidence loss (the objectness of the box). Classification loss $$ \\sum_{i=0}^{S^2} \\mathbf{1}_i^{obj} \\cdot{ \\sum _{cc\\in{classes}} \\left( p _{i}(cc) - \\hat{p} _i(cc) \\right)^2} $$ where $\\mathbf{1}_i^{obj} = 1$ if an object appears in cell $i$, otherwise 0;\n$\\hat{p} _i(cc)$ denotes the conditional class probability for class $cc$ in cell $i$.\nLocalization loss \\begin{aligned} \\lambda _{coord} \\sum _{i=0}^{S^2} \\sum _{j=0}^{B} \\mathbf{1} _{ij}^{obj} \\left[ (x _i - \\hat{x _i})^2 + (y _i - \\hat{y} _i)^2 \\right] \\ + \\lambda _{coord} \\sum _{i=0}^{S^2} \\sum _{j=0}^{B} \\mathbf{1} _{ij}^{obj} \\left[ (\\sqrt{w _i} - \\sqrt{\\hat{w} _i} )^2 + (\\sqrt{h _i} - \\sqrt{\\hat{h} _i} )^2 \\right] \\end{aligned}\nwhere $\\mathbf{1}_{ij}^{obj} = 1$ if the $j$th boundary box in cell $i$ is responsible for detecting object, otherwise 0;\n$\\lambda_{coord}$ increases the weight for the loss in the boundary box coordinates.\nYOLO predicts the square root of bounding box width and height in order to differentiate large and small boxes. By setting $\\lambda_{coord}$ (default: 5), we put more emphasis on the boundary box accuracy.\nConfidence loss If an object is detected in the box, the confidence loss is:\n$$ \\sum _{i=0}^{S^2} \\sum _{j=0}^{B} \\mathbf{1} _{ij}^{obj} \\left( C _i - \\hat{C} _i \\right)^2 $$\nwhere $\\mathbf{1}_{ij}^{obj} = 1$ if the $j$th boundary box in cell $i$ is responsible for detecting the object, otherwise 0;\n$\\hat{C} _i$ is the box confidence score of the box $j$ in cell $i$.\nHowever, if an object is not detected:\n$$ \\lambda _{backg} \\sum _{i=0}^{S^2} \\sum _{j=0}^{B} \\mathbf{1} _{ij}^{backg} \\left( C _i - \\hat{C} _i \\right)^2 $$\nwhere $\\mathbf{1} _{ij}^{backg} $ is the complement of $ \\mathbf{1} _{ij}^{obj}$.\n$\\hat{C} _i$ is the box confidence score of the box $j$ in cell $i$.\n$\\lambda _{backg}$ weights down the loss when detecting background.\nAs most boxes do not contain any objects, we weight the loss down by a factor $\\lambda _{backg}$ (default: 0.5) to balance the weight.\n5. Inference: Non-maximal Suppression Next, we multiply all these class scores with bounding box confidence and get class scores for different boudning boxes. So output is $7\\times 7\\times 2 = 98$. Then we set a threshold value of scores and sort them descendingly. Non-max supressing alogrithm is used to set score to zero for redundant boxes.\nFor example, dog score for bbox1 as 0.5 and bbox 2 as 0.3. We take an Intersection over Union (IOU) of these values and if the value is greater than 0.5, we will set the value for box2 as zero, otherwise continue to the next box.\n","date":1534736398,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534736398,"objectID":"6b5fe54d9f0eadafbb859372afd7fd49","permalink":"https://zhanghanduo.github.io/post/yolo1/","publishdate":"2018-08-20T11:39:58+08:00","relpermalink":"/post/yolo1/","section":"post","summary":"You Only Look Once (YOLO) is an object detection system targeted for real-time processing. There are three versions of YOLO: YOLO, YOLOv2 (and YOLO9000) and YOLOv3. For this article, we mainly focus on YOLO first stage.","tags":["ML","Notes"],"title":"You only look once (YOLO) -- (1)","type":"post"},{"authors":["Handuo"],"categories":[],"content":"Why use CLion? Better indexing and intelligence hints for C++ than Eclipse and QtCreator-desktop. Free for students. Also integrate PyCharm already. Good Git integration (although I am still used to commandline git). I really like the code inspection clang-tidy function which makes the code style more modern. Initial set-up Highly recommend you to add source \u0026lt;CATKIN_WORKSPACE_DIR\u0026gt;/devel/setup.bash to the end of ~/.bashrc or ~/.zshrc (Depends you use bash or zsh). So when you type\necho $ROS_PACKAGE_PATH you can find both \u0026lt;CATKIN_WORKSPACE_DIR\u0026gt;/src and /opt/ros/\u0026lt;ROS_DIST\u0026gt;/share. So next time you open any terminal, your cmakelist can find the package of catkin.\nMethod 1: Launch CLion via terminal sh \u0026lt;CLION_INSTALL_DIR\u0026gt;/bin/clion.sh Recommend you to make alias for this command in ~/.bashrc.\nMethod 2: Launch CLion via app icon on sidebar Just edit /usr/share/applications/jetbrains-clion.desktop. If it does not exist, open up Clion and hit Tools \u0026gt; Create Desktop Entry first. Here I give an example and if you are using zsh, just change bash to zsh.\n[Desktop Entry] Version=1.0 Type=Application Name=CLion Icon=XXX/clion-2017.2.3/bin/clion.svg Exec=bash -i -c \u0026#34;XXX/clion-2017.2.3/bin/clion.sh\u0026#34; %f Comment=The Drive to Develop Categories=Development;IDE; Terminal=false StartupWMClass=jetbrains-clion Settings In CLion, you can set the Toolchain dialog (including CMake, C++ compiler, gdb) so it can be consistent with your current system-level toolchain.\nAlso you can set the pass any envrionment variables and parameters to CMake in CLion by using the CMake dialog.\nC++ tool chain settings Cmake settings CLion builds your project in cmake-build-debug by default. If you want to change that, you could easily set:\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY \u0026#34;my_dir\u0026#34;) Or change the build output directory in the CMake dialog as well.\nIn addition, the Run/Debug Configurations dialog in the right up corner allows you to set program execution arguments, working directory, and environment variables.\nopen debug \u0026amp; run setting modify execution arguments Now try to build \u0026amp; run your ROS project! I hope this could provide you with better experience on project development.\n","date":1525313249,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525313249,"objectID":"a863f083b14d41a29afca3083db6781d","permalink":"https://zhanghanduo.github.io/post/clion/","publishdate":"2018-05-03T10:07:29+08:00","relpermalink":"/post/clion/","section":"post","summary":"Why use CLion? Better indexing and intelligence hints for C++ than Eclipse and QtCreator-desktop. Free for students. Also integrate PyCharm already. Good Git integration (although I am still used to commandline git).","tags":["Tips","Tools","ROS"],"title":"CLion for catkin projects","type":"post"},{"authors":["Handuo"],"categories":["Network architecture"],"content":"Today I didnâ€™t have the mood to continue my work on map merging of different cameras. So I read the paper from DeepMind of Learned Deformation Stability in Convolutional Neural Networks recommended by Wang Chen.\n1. Convolution Operation Convolution operation is typically denoted with an asterisk1: $$ s(t)=(x*w)(t) $$ In Convolutional network terminology, the x is referred to as the input, and the w as the kernel. The output is sometimes referred to as the feature map.\nConvolution leverages three ideas that help improve the ML system: sparse interactions, parameter sharing and equivariant representations. Moreover, convolution provides a means for working with inputs of variable size.\n- Sparse interactions Also called sparse connectivity or sparse weights, makes the kernel smaller than the input. By detecting small, meaningful features such as edges with kernels (tens or hundreds of pixels), we donâ€™t need to process all the input pixels(millions), which means much fewer parameters.\nSparse connectivity Parameter sharing - Parameter sharing Refers to using the same parameter for more than one function in a model, so a network has tied weights. This reduces the storage requirements of the model to k parameters.\n- Equivariance to translation But not to other transformations like scale or rotation change.\n2. Pooling In all cases, pooling helps to make the representation become approximately invariant to small translations of the input. Pooling over spatial regions produces invariance to translation, but if we pool over the outputs of separately parametrized convolutions, the features can learn which transformations to become invariant to.\nBecause pooling summarizes the responses over a whole neighborhood, it is possible to use fewer pooling units than detector units, by reporting summary statistics for pooling regions spaced k pixels apart rather than 1 pixel apart.\nThis article tries to analyze the relationship between the pooling layers and deformation stability in CNN based on the paper Learned Deformation Stability in Convolutional Neural Networks.\nThe paper tries to address the following questions:\nDoes pooling have an effect on the learned deformation stability?\nIs deformation stability achieved in the absence of pooling?\nHow can deformation stability be achieved in the absence of pooling?\nTraditional way of thinking pooling layer is that it is useful in two reasons:\nBy eliminating non-maximal (for max-pooling), it reduces computation for upper layers. It porvides a form of translation invariance. Imagine cascading a max-pooling layer with a convolutional layer. There are 8 directions in which one can translate the input image by a single pixel. If max-pooling is done over a $2\\times2$ region, 3 out of these 8 possible configurations will produce exactly the same output at the conv layer. For $3\\times3$ windows, this jumps to $\\frac{5}{8}$. Then the author first defines the invariance to transformations of an input X that do not affect the output Y:\n$$ P(Y|\\tau(X))=P(Y|X) \\forall \\tau \\in T $$\nThen he defines the measurement of sensitivity to deformation to evaluate this invariance.\nFor a representation r mapping from input image to some layer of a CNN, the measurement is:\n$$ \\frac{dcos(r(x), r(\\tau(x))}{median(dcos(r(x), r(y)))} $$ where dcos is the cosine distance. That is we normalize distances by the median distance between randomly selected images from the original dataset.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville, Deep Learning, 2016.Â â†©ï¸\n","date":1525227378,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525227378,"objectID":"f6831cb2a7e50df6a3d5e8235ce52c56","permalink":"https://zhanghanduo.github.io/post/pooling/","publishdate":"2018-05-02T10:16:18+08:00","relpermalink":"/post/pooling/","section":"post","summary":"Today I didnâ€™t have the mood to continue my work on map merging of different cameras. So I read the paper from DeepMind of Learned Deformation Stability in Convolutional Neural Networks recommended by Wang Chen.","tags":["ML","Notes"],"title":"Pooling Layer in CNN (1)","type":"post"},{"authors":["Han Wang","Rui Jiang","Handuo Zhang","Shuzhi Sam Ge"],"categories":null,"content":"","date":1525067291,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525067291,"objectID":"8ee18b487c2abe2bf6c873dc202f98d4","permalink":"https://zhanghanduo.github.io/publication/ahrs/","publishdate":"2018-04-30T13:48:11+08:00","relpermalink":"/publication/ahrs/","section":"publication","summary":"In this paper, heading reference-assisted pose estimation (HRPE) has been proposed to compensate inherent drift of visual odometry (VO) on ground vehicles, where an estimation error is prone to grow while the vehicle is making turns or in environments with poor features. By introducing a particular orientation as ``heading reference,'' a pose estimation framework has been presented to incorporate measurements from heading reference sensors into VO. A graph formulation is then proposed to represent the pose estimation problem under the commonly used graph optimization model. Simulations and experiments on KITTI data set and our self-collected sequences have been conducted to verify the accuracy and robustness of the proposed scheme. KITTI sequences and manually generated heading measurement with Gaussian noises are used in simulation, where rotational drift error is observed to be bounded. Compared with a pure VO, the proposed approach greatly reduces average translational localization error from 153.85 to 24.29 m and 23.80 m in self-collected stereo visual sequences with traveling distance over 4.5 km at the processing rates of 19.7 and 11.1 Hz, for the loosely coupled and tightly coupled models, respectively.","tags":[],"title":"Heading Reference-Assisted Pose Estimation for Ground Vehicles","type":"publication"},{"authors":["Handuo"],"categories":[],"content":" ORBSLAM vs SVO\nSVO ï¼ˆå•ç›®ï¼‰\nä¼˜ç‚¹ï¼š é€Ÿåº¦æå¿«ï¼Œ100å¤šå¸§ï¼Œåœ¨ä½ç«¯è®¡ç®—æœºä¸Šä¹Ÿèƒ½è¾¾åˆ°å®æ—¶æ€§ã€‚è¿½è¸ªå’Œå»ºå›¾ä¸¤ä¸ªçº¿ç¨‹ï¼Œè¿½è¸ªçº¿ç¨‹å’Œptamæˆ–è€…orbslamå¾ˆåƒï¼Œä¹Ÿæ˜¯å»ºç«‹è¯¯å·®é¡¹ï¼Œç„¶årefineå’ŒBAï¼ŒåŒºåˆ«å°±æ˜¯ç”¨çš„ç›´æ¥æ³•çš„Image alignmentè€Œä¸æ˜¯ç‰¹å¾ç‚¹å‡ ä½•ä½ç½®ä¿¡æ¯ã€‚\nç¼ºç‚¹ï¼šåªæ˜¯é‡Œç¨‹è®¡ï¼Œæ²¡æœ‰åç«¯ä¼˜åŒ–å’Œå›ç¯æ£€æµ‹ï¼Œæ‰€ä»¥ç´¯è®¡è¯¯å·®è¾ƒå¤§ï¼Œè€Œä¸”ä¸€æ—¦ä¸¢äº†å°±æŒ‚äº†ï¼Œæ²¡æ³•é‡å®šä½; è€Œä¸”åœ¨è®¾è®¡çš„æ—¶å€™é’ˆå¯¹çš„æ˜¯ä¿¯è§†çš„æ— äººæœºæ‘„åƒå¤´ï¼Œå¯¹äºå¹³è§†çš„æ‘„åƒæœºæ•ˆæœå¾ˆå·®; æ‹¥æœ‰ç›´æ¥æ³•çš„æ‰€æœ‰ç¼ºç‚¹ï¼šæ€•å…‰ç…§å˜åŒ–ï¼Œæ€•æ¨¡ç³Šï¼Œæ€•å¤§è¿åŠ¨\nsvoè¿˜æœ‰ä¸€ä¸ªä¸æ˜¯ç¼ºç‚¹çš„ç¼ºç‚¹ï¼Œå®ƒå¼€æºçš„ä»£ç æœ‰å¥½å¤šå¥½å¤šå‘ï¼Œä½œè€…å¾ˆå¤šç§è´§æ•…æ„æ²¡æœ‰å†™è¿›å¼€æºçš„ä»£ç é‡Œé¢ï¼Œæ‰€ä»¥å®é™…ç”¨çš„æ—¶å€™æœ‰å¾ˆå¤šé—®é¢˜ï¼Œéœ€è¦è‡ªå·±æ ¹æ®æƒ…å†µæ”¹è¿›ã€‚\nOrbSLAM ä¼˜ç‚¹ï¼šæ”¯æŒå•ç›®ï¼ŒåŒç›®ï¼ŒRGBDï¼Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„ç³»ç»Ÿï¼ŒåŒ…å«äº†é‡Œç¨‹è®¡ï¼Œç‰¹å¾ç‚¹å»ºå›¾BAï¼Œå›ç¯æ£€æµ‹ä¸‰ä¸ªç‹¬ç«‹çº¿ç¨‹ï¼Œåœ¨i7ä¸Šå¤§æ¦‚15ï½20hzï¼ˆè·Ÿè¾“å…¥å›¾åƒå¤§å°ä»¥åŠå‚æ•°è®¾ç½®æœ‰å…³ï¼‰ï¼Œç²¾ç¡®åº¦åœ¨è¿‘å¹´æ¥å±äºæ¯”è¾ƒé«˜çš„äº†ã€‚ç»¼åˆèƒ½åŠ›æœ€å¼ºã€‚\nç¼ºç‚¹ï¼šORBçš„æå–ä»¥åŠmatchè€—æ—¶è¾ƒå¤§ï¼Œè¿‡å¿«çš„æ—‹è½¬å¯èƒ½ä¼šä¸¢å¤±ã€‚è€Œä¸”å› ä¸ºä¸‰ä¸ªçº¿ç¨‹ä¼šç»™CPUå¸¦æ¥è¾ƒå¤§è´Ÿæ‹…ï¼ŒåŸºæœ¬æ²¡åŠæ³•å†è·‘å…¶ä»–å¤§å‹ç®—æ³•äº†ã€‚å¯¹äºåœºæ™¯ç‰¹å¾ç‚¹ä¸°å¯Œè¦æ±‚é«˜ï¼ŒæŸäº›åœºæ™¯å¦‚æœæ²¡ä»€ä¹ˆç‰¹å¾å¯èƒ½å°±ä¼šå¤±è´¥æˆ–è€…ä¸å‡†ç¡®ã€‚\nèƒ½å¤Ÿè·Ÿorbslam pkçš„æ˜¯æ¯”è¾ƒæ–°çš„DSOç®—æ³•ï¼ŒSVOé™¤äº†é€Ÿåº¦åŸºæœ¬å„ä¸ªæ–¹é¢è¢«åŠæ‰“ã€‚\nDepth acquirement\nå…ˆè¯´svoçš„æ·±åº¦æ»¤æ³¢å™¨ï¼Œå±äºæ¸è¿›å¼çš„ä¸‰ç»´é‡å»ºï¼Œä¸æ˜¯å•çº¯çš„æ±‚å–keypointçš„æ·±åº¦ï¼Œè€Œæ˜¯è¦ç»´æŠ¤å€™é€‰ç‚¹seedçš„æ·±åº¦åˆ†å¸ƒï¼Œä»æœªçŸ¥åˆ°ç²—ç•¥åˆ°æ”¶æ•›ï¼Œæ”¶æ•›äº†æ‰æ”¾åˆ°åœ°å›¾ä¸­å…±è¿½è¸ªçº¿ç¨‹ä½¿ç”¨ã€‚å®é™…ä¸Šæ”¶æ•›è¾ƒæ…¢ï¼Œç»“æœä¸¥é‡ä¾èµ–äºå‡†ç¡®çš„ä½å§¿ä¼°è®¡ï¼Œå› æ­¤ç›¸æ¯”äºç‰¹å¾ç‚¹æ³•çš„BAæ²¡æœ‰ä»€ä¹ˆä¼˜åŠ¿ã€‚ 2ï¼‰å†è¯´orbslamçš„BAï¼Œç›¸æ¯”æ¥è¯´è®¡ç®—é‡æ›´å¤§ï¼Œä½†æ˜¯å› ä¸ºæ˜¯frame-frameä»¥åŠlocal mapçš„ä¸¤æ¬¡ä¼˜åŒ–ï¼Œç²¾ç¡®åº¦åº”è¯¥é«˜äºsvoçš„æ»¤æ³¢å™¨ã€‚\nç°åº¦ä¸å˜æ€§å‡è®¾\nç°åº¦å€¼ä¸å˜æ˜¯å¾ˆå¼ºçš„å‡è®¾ã€‚å¦‚æœç›¸æœºæ˜¯è‡ªåŠ¨æ›å…‰çš„ï¼Œå½“å®ƒè°ƒæ•´æ›å…‰å‚æ•°æ—¶ï¼Œä¼šä½¿å¾—å›¾åƒæ•´ä½“å˜äº®æˆ–å˜æš—ã€‚å…‰ç…§å˜åŒ–æ—¶ä¹Ÿä¼šå‡ºç°è¿™ç§æƒ…å†µã€‚ç‰¹å¾ç‚¹æ³•å¯¹å…‰ç…§å…·æœ‰ä¸€å®šçš„å®¹å¿æ€§ï¼Œè€Œç›´æ¥æ³•ç”±äºè®¡ç®—ç°åº¦é—´çš„å·®å¼‚ï¼Œæ•´ä½“ç°åº¦å˜åŒ–ä¼šç ´åç°åº¦ä¸å˜å‡è®¾ï¼Œä½¿ç®—æ³•å¤±è´¥ã€‚\nsvoä¸ç›´æ¥å–æŸä¸€ä¸ªåƒç´ ï¼Œè€Œæ˜¯4x4çš„patchï¼Œæˆ‘ä»¬å¯¹å¤šä¸ªç‚¹è¿›è¡Œæœ€å°åŒ–å…‰åº¦è¯¯å·®æ¥è®¡ç®—çš„ã€‚æ‰€ä»¥æ²¡æœ‰æ˜ç¡®çš„thresholdï¼Œå¦‚æœæŸä¸€ä¸¤ä¸ªç‚¹å…‰ç…§å‘ç”Ÿå˜åŒ–è¿˜å¥½ï¼Œå°‘æ•°æœä»å¤šæ•°ã€‚å¦‚æœå‘ç”Ÿå¾®å°å˜åŒ–äº†ï¼Œä½†æ˜¯åªè¦é€‰å–çš„ç‰¹å¾åŒºåŸŸçš„å…‰ç…§è·Ÿå…¶ä»–åŒºåŸŸåŒºåˆ«å¾ˆå¤§ï¼Œä¹Ÿæ˜¯å¯ä»¥å¿å—çš„ï¼Œæ¯•ç«Ÿæœ€å°åŒ–è€Œä¸å¼ºæ±‚å…‰ç…§çš„è¯¯å·®ä¸º0ã€‚æ‰€ä»¥è¯´æœ‰ä¸€å®šçš„é²æ£’æ€§ï¼Œè™½ç„¶è·Ÿç‰¹å¾ç‚¹æ³•æ²¡æ³•æ¯”ã€‚\nVOé™æ­¢çš„æ—¶å€™ï¼Œä¼šæ¼‚å—\nä¸€èˆ¬æƒ…å†µä¸‹VOä¸€èˆ¬ä¸ä¼šæ¼‚çš„ å¦‚æœæ˜¯VIOçš„è¯æ˜¯æœ‰å¯èƒ½çš„ï¼Œä¸€èˆ¬æœ‰å‡ ä¸ªåŸå› :\ncalibrationåšçš„ä¸å¤Ÿå‡† åˆå§‹åŒ–çš„æ—¶å€™æ‘„åƒå¤´å°½é‡å†²ç€ç‰¹å¾ç‚¹ä¸°å¯Œçš„åœ°æ–¹ï¼Œå¹¶ç¼“æ…¢ç§»åŠ¨æ‘„åƒå¤´ï¼Œå……åˆ†åˆå§‹åŒ– IMUå¦‚æœè´¨é‡ä¸å¥½ä¹Ÿä¼šè½»å¾®çš„æ¼‚ï¼Œä½†æ˜¯ä¸ä¸¥é‡\n","date":1522750141,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522750141,"objectID":"5b2e4f676166e474ddde9c2c3ae6fa10","permalink":"https://zhanghanduo.github.io/post/questions_slam/","publishdate":"2018-04-03T18:09:01+08:00","relpermalink":"/post/questions_slam/","section":"post","summary":"ORBSLAM vs SVO\nSVO ï¼ˆå•ç›®ï¼‰\nä¼˜ç‚¹ï¼š é€Ÿåº¦æå¿«ï¼Œ100å¤šå¸§ï¼Œåœ¨ä½ç«¯è®¡ç®—æœºä¸Šä¹Ÿèƒ½è¾¾åˆ°å®æ—¶æ€§ã€‚è¿½è¸ªå’Œå»ºå›¾ä¸¤ä¸ªçº¿ç¨‹ï¼Œè¿½è¸ªçº¿ç¨‹å’Œptamæˆ–è€…orbslamå¾ˆåƒï¼Œä¹Ÿæ˜¯å»ºç«‹è¯¯å·®é¡¹ï¼Œç„¶årefineå’ŒBAï¼ŒåŒºåˆ«å°±æ˜¯ç”¨çš„ç›´æ¥æ³•çš„Image alignmentè€Œä¸æ˜¯ç‰¹å¾ç‚¹å‡ ä½•ä½ç½®ä¿¡æ¯ã€‚\nç¼ºç‚¹ï¼šåªæ˜¯é‡Œç¨‹è®¡ï¼Œæ²¡æœ‰åç«¯ä¼˜åŒ–å’Œå›ç¯æ£€æµ‹ï¼Œæ‰€ä»¥ç´¯è®¡è¯¯å·®è¾ƒå¤§ï¼Œè€Œä¸”ä¸€æ—¦ä¸¢äº†å°±æŒ‚äº†ï¼Œæ²¡æ³•é‡å®šä½; è€Œä¸”åœ¨è®¾è®¡çš„æ—¶å€™é’ˆå¯¹çš„æ˜¯ä¿¯è§†çš„æ— äººæœºæ‘„åƒå¤´ï¼Œå¯¹äºå¹³è§†çš„æ‘„åƒæœºæ•ˆæœå¾ˆå·®; æ‹¥æœ‰ç›´æ¥æ³•çš„æ‰€æœ‰ç¼ºç‚¹ï¼šæ€•å…‰ç…§å˜åŒ–ï¼Œæ€•æ¨¡ç³Šï¼Œæ€•å¤§è¿åŠ¨\nsvoè¿˜æœ‰ä¸€ä¸ªä¸æ˜¯ç¼ºç‚¹çš„ç¼ºç‚¹ï¼Œå®ƒå¼€æºçš„ä»£ç æœ‰å¥½å¤šå¥½å¤šå‘ï¼Œä½œè€…å¾ˆå¤šç§è´§æ•…æ„æ²¡æœ‰å†™è¿›å¼€æºçš„ä»£ç é‡Œé¢ï¼Œæ‰€ä»¥å®é™…ç”¨çš„æ—¶å€™æœ‰å¾ˆå¤šé—®é¢˜ï¼Œéœ€è¦è‡ªå·±æ ¹æ®æƒ…å†µæ”¹è¿›ã€‚\nOrbSLAM ä¼˜ç‚¹ï¼šæ”¯æŒå•ç›®ï¼ŒåŒç›®ï¼ŒRGBDï¼Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„ç³»ç»Ÿï¼ŒåŒ…å«äº†é‡Œç¨‹è®¡ï¼Œç‰¹å¾ç‚¹å»ºå›¾BAï¼Œå›ç¯æ£€æµ‹ä¸‰ä¸ªç‹¬ç«‹çº¿ç¨‹ï¼Œåœ¨i7ä¸Šå¤§æ¦‚15ï½20hzï¼ˆè·Ÿè¾“å…¥å›¾åƒå¤§å°ä»¥åŠå‚æ•°è®¾ç½®æœ‰å…³ï¼‰ï¼Œç²¾ç¡®åº¦åœ¨è¿‘å¹´æ¥å±äºæ¯”è¾ƒé«˜çš„äº†ã€‚ç»¼åˆèƒ½åŠ›æœ€å¼ºã€‚\nç¼ºç‚¹ï¼šORBçš„æå–ä»¥åŠmatchè€—æ—¶è¾ƒå¤§ï¼Œè¿‡å¿«çš„æ—‹è½¬å¯èƒ½ä¼šä¸¢å¤±ã€‚è€Œä¸”å› ä¸ºä¸‰ä¸ªçº¿ç¨‹ä¼šç»™CPUå¸¦æ¥è¾ƒå¤§è´Ÿæ‹…ï¼ŒåŸºæœ¬æ²¡åŠæ³•å†è·‘å…¶ä»–å¤§å‹ç®—æ³•äº†ã€‚å¯¹äºåœºæ™¯ç‰¹å¾ç‚¹ä¸°å¯Œè¦æ±‚é«˜ï¼ŒæŸäº›åœºæ™¯å¦‚æœæ²¡ä»€ä¹ˆç‰¹å¾å¯èƒ½å°±ä¼šå¤±è´¥æˆ–è€…ä¸å‡†ç¡®ã€‚\nèƒ½å¤Ÿè·Ÿorbslam pkçš„æ˜¯æ¯”è¾ƒæ–°çš„DSOç®—æ³•ï¼ŒSVOé™¤äº†é€Ÿåº¦åŸºæœ¬å„ä¸ªæ–¹é¢è¢«åŠæ‰“ã€‚\nDepth acquirement\nå…ˆè¯´svoçš„æ·±åº¦æ»¤æ³¢å™¨ï¼Œå±äºæ¸è¿›å¼çš„ä¸‰ç»´é‡å»ºï¼Œä¸æ˜¯å•çº¯çš„æ±‚å–keypointçš„æ·±åº¦ï¼Œè€Œæ˜¯è¦ç»´æŠ¤å€™é€‰ç‚¹seedçš„æ·±åº¦åˆ†å¸ƒï¼Œä»æœªçŸ¥åˆ°ç²—ç•¥åˆ°æ”¶æ•›ï¼Œæ”¶æ•›äº†æ‰æ”¾åˆ°åœ°å›¾ä¸­å…±è¿½è¸ªçº¿ç¨‹ä½¿ç”¨ã€‚å®é™…ä¸Šæ”¶æ•›è¾ƒæ…¢ï¼Œç»“æœä¸¥é‡ä¾èµ–äºå‡†ç¡®çš„ä½å§¿ä¼°è®¡ï¼Œå› æ­¤ç›¸æ¯”äºç‰¹å¾ç‚¹æ³•çš„BAæ²¡æœ‰ä»€ä¹ˆä¼˜åŠ¿ã€‚ 2ï¼‰å†è¯´orbslamçš„BAï¼Œç›¸æ¯”æ¥è¯´è®¡ç®—é‡æ›´å¤§ï¼Œä½†æ˜¯å› ä¸ºæ˜¯frame-frameä»¥åŠlocal mapçš„ä¸¤æ¬¡ä¼˜åŒ–ï¼Œç²¾ç¡®åº¦åº”è¯¥é«˜äºsvoçš„æ»¤æ³¢å™¨ã€‚\nç°åº¦ä¸å˜æ€§å‡è®¾\nç°åº¦å€¼ä¸å˜æ˜¯å¾ˆå¼ºçš„å‡è®¾ã€‚å¦‚æœç›¸æœºæ˜¯è‡ªåŠ¨æ›å…‰çš„ï¼Œå½“å®ƒè°ƒæ•´æ›å…‰å‚æ•°æ—¶ï¼Œä¼šä½¿å¾—å›¾åƒæ•´ä½“å˜äº®æˆ–å˜æš—ã€‚å…‰ç…§å˜åŒ–æ—¶ä¹Ÿä¼šå‡ºç°è¿™ç§æƒ…å†µã€‚ç‰¹å¾ç‚¹æ³•å¯¹å…‰ç…§å…·æœ‰ä¸€å®šçš„å®¹å¿æ€§ï¼Œè€Œç›´æ¥æ³•ç”±äºè®¡ç®—ç°åº¦é—´çš„å·®å¼‚ï¼Œæ•´ä½“ç°åº¦å˜åŒ–ä¼šç ´åç°åº¦ä¸å˜å‡è®¾ï¼Œä½¿ç®—æ³•å¤±è´¥ã€‚\nsvoä¸ç›´æ¥å–æŸä¸€ä¸ªåƒç´ ï¼Œè€Œæ˜¯4x4çš„patchï¼Œæˆ‘ä»¬å¯¹å¤šä¸ªç‚¹è¿›è¡Œæœ€å°åŒ–å…‰åº¦è¯¯å·®æ¥è®¡ç®—çš„ã€‚æ‰€ä»¥æ²¡æœ‰æ˜ç¡®çš„thresholdï¼Œå¦‚æœæŸä¸€ä¸¤ä¸ªç‚¹å…‰ç…§å‘ç”Ÿå˜åŒ–è¿˜å¥½ï¼Œå°‘æ•°æœä»å¤šæ•°ã€‚å¦‚æœå‘ç”Ÿå¾®å°å˜åŒ–äº†ï¼Œä½†æ˜¯åªè¦é€‰å–çš„ç‰¹å¾åŒºåŸŸçš„å…‰ç…§è·Ÿå…¶ä»–åŒºåŸŸåŒºåˆ«å¾ˆå¤§ï¼Œä¹Ÿæ˜¯å¯ä»¥å¿å—çš„ï¼Œæ¯•ç«Ÿæœ€å°åŒ–è€Œä¸å¼ºæ±‚å…‰ç…§çš„è¯¯å·®ä¸º0ã€‚æ‰€ä»¥è¯´æœ‰ä¸€å®šçš„é²æ£’æ€§ï¼Œè™½ç„¶è·Ÿç‰¹å¾ç‚¹æ³•æ²¡æ³•æ¯”ã€‚\nVOé™æ­¢çš„æ—¶å€™ï¼Œä¼šæ¼‚å—\nä¸€èˆ¬æƒ…å†µä¸‹VOä¸€èˆ¬ä¸ä¼šæ¼‚çš„ å¦‚æœæ˜¯VIOçš„è¯æ˜¯æœ‰å¯èƒ½çš„ï¼Œä¸€èˆ¬æœ‰å‡ ä¸ªåŸå› :\ncalibrationåšçš„ä¸å¤Ÿå‡† åˆå§‹åŒ–çš„æ—¶å€™æ‘„åƒå¤´å°½é‡å†²ç€ç‰¹å¾ç‚¹ä¸°å¯Œçš„åœ°æ–¹ï¼Œå¹¶ç¼“æ…¢ç§»åŠ¨æ‘„åƒå¤´ï¼Œå……åˆ†åˆå§‹åŒ– IMUå¦‚æœè´¨é‡ä¸å¥½ä¹Ÿä¼šè½»å¾®çš„æ¼‚ï¼Œä½†æ˜¯ä¸ä¸¥é‡","tags":["Tips"],"title":"SVOç›¸å…³é—®é¢˜","type":"post"},{"authors":["Chen Wang","Handuo Zhang","Thien-Minh Nguyen","Lihua Xie"],"categories":null,"content":"","date":1506253007,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506253007,"objectID":"7baa5ad8cf2cf82a084f3af951a5e57a","permalink":"https://zhanghanduo.github.io/publication/ultra-wideband_localization/","publishdate":"2017-09-24T19:36:47+08:00","relpermalink":"/publication/ultra-wideband_localization/","section":"publication","summary":"This paper proposes an ultra-wideband (UWB) aided localization and mapping system that leverages on inertial sensor and depth camera. Inspired by the fact that visual odometry (VO) system, regardless of its accuracy in the short term, still faces challenges with accumulated errors in the long run or under unfavourable environments, the UWB ranging measurements are fused to remove the visual drift and improve the robustness. A general framework is developed which consists of three parallel threads, two of which carry out the visualinertial odometry (VIO) and UWB localization respectively. The other mapping thread integrates visual tracking constraints into a pose graph with the proposed smooth and virtual range constraints, such that a bundle adjustment is performed to provide robust trajectory estimation. Experiments show that the proposed system is able to create dense drift-free maps in real-time even running on an ultra-low power processor in featureless environments.","tags":["SLAM"],"title":"Ultra-wideband aided fast localization and mapping system","type":"publication"},{"authors":["Handuo Zhang","Karunasekera Hasith","Han Wang"],"categories":null,"content":"","date":1499081807,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499081807,"objectID":"b9b594b24f375081dc1841261d605a97","permalink":"https://zhanghanduo.github.io/publication/hybrid/","publishdate":"2017-07-03T19:36:47+08:00","relpermalink":"/publication/hybrid/","section":"publication","summary":"In visual simultaneous localization and mapping (SLAM) field, especially for feature based stereo-SLAM, data association is one of the most important and time-consuming sub-tasks. In this paper, we investigate the roles of different measured features during the data association process and present a new hybrid feature parametrization approach for stereo SLAM, which only selects a subset of the matched features that contributes most and treats nearby and distant features separately with different parametrization. We formulate a pipeline to filter, store and track the features which saves time for further state estimation. For different types of features on manifold and Euclidean space we apply corresponding designed maximum likelihood estimator with quadratic constraints and thus get a near-optimal estimation. Experimental results on EuRoC dataset and real tests show that our proposed algorithm leads to accurate state estimation with big progress in consistency.","tags":["SLAM"],"title":"A Hybrid Feature Parametrization for Improving Stereo-SLAM Consistency","type":"publication"},{"authors":["Karunasekera Hasith","Handuo Zhang","Tao Xi","Han Wang"],"categories":null,"content":"","date":1499047096,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499047096,"objectID":"5885177f9e6e5d0d55f41a28a1ed78e5","permalink":"https://zhanghanduo.github.io/publication/stereo_negative/","publishdate":"2017-07-03T09:58:16+08:00","relpermalink":"/publication/stereo_negative/","section":"publication","summary":"Autonomous robots also known as Unmanned Ground Vehicles (UGV) is of broad and current interest, in both research and industry. Many studies have been done in stereo vision based UGV vision in positive obstacle detection and avoidance, but negative obstacle detection and avoidance remains less explored. In this paper, we propose a new method to detect negative obstacles incorporating the knowledge gained from stereo vision and detecting saliency on the ground plane. Experimental results show that our method works well on detecting negative obstacles.","tags":[],"title":"Stereo Vision based Negative Obstacle Detection","type":"publication"},{"authors":["Shiping Wang","Handuo Zhang","Han Wang"],"categories":null,"content":"","date":1487936207,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487936207,"objectID":"03aba8cb849408d2a389dd2ba56bb27d","permalink":"https://zhanghanduo.github.io/publication/object-co-segmentation-via-weakly-supervised-data-fusion/","publishdate":"2017-02-24T19:36:47+08:00","relpermalink":"/publication/object-co-segmentation-via-weakly-supervised-data-fusion/","section":"publication","summary":"Object co-segmentation aims to simultaneously segment common regions of interest from multiple images. It is of great importance to image classification, object recognition and image retrieval. One way to extract similar objects shared by multiple images is to construct a correlation function between image regions. In this paper, object co-segmentation is addressed based on weakly supervised data fusion.  First, we integrate the image boundary information into weakly supervised clustering by adopting an efficient image segmentation algorithm with proved convergence. Feature learning as well as clustering are also incorporated into the proposed algorithm to establish a unified framework so that an optimal feature subspace with clustering-oriented methods is provided. Second, the shared object from multiple images is regarded as the procedure to search objects from heterogeneous data sources, which is formalized as data fusion problems. Using data fusion techniques, we present a novel method to evaluate the similarity between images, which facilitates the use of similar objects from multiple images. Finally, the two proposed object segmentation and co-segmentation algorithms are verified through publicly available datasets MSRA1000 and iCoseg. Experiments demonstrate that both algorithms are capable to achieve superior or comparable performance over the compared state-of-the-art segmentation methods in all tested datasets.","tags":[],"title":"Object co-segmentation via weakly supervised data fusion","type":"publication"}]